{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "w_ch=adxi\n",
    "d=(y-yhat)output_prime=(y-yhat)output_prime(sum(wixi))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# linear comb by the node\n",
    "h=x[0]*weights[0]+x[1]*weights[1]\n",
    "h=np.dot(x,weights)\n",
    "\n",
    "# the neural network output\n",
    "nn_output=sigmoid(h)\n",
    "\n",
    "# output error(y-yhat)\n",
    "error=y-nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad=sigmoid_prime(h)\n",
    "\n",
    "# error term(d)\n",
    "error_term=error*output_grad\n",
    "\n",
    "# gradient descent step\n",
    "del_w=[learnrate*error_term*x[0], learnrate*error_term*x[1]]\n",
    "del_w=learnrate*error_term*x\n",
    "\n",
    "# update weights\n",
    "weights += learnrate*del_w/n_records\n",
    "\n",
    "error term for hidden unit is\n",
    "\n",
    "sum(W*error_term_out*output_grad_h)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set the weight step to zero: \\Delta w_i = 0Δw \n",
    "i\n",
    "​\t =0\n",
    "For each record in the training data:\n",
    "Make a forward pass through the network, calculating the output \\hat y = f(\\sum_i w_i x_i) \n",
    "y\n",
    "^\n",
    "​\t =f(∑ \n",
    "i\n",
    "​\t w \n",
    "i\n",
    "​\t x \n",
    "i\n",
    "​\t )\n",
    "Calculate the error term for the output unit, \\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)δ=(y− \n",
    "y\n",
    "^\n",
    "​\t )∗f \n",
    "′\n",
    " (∑ \n",
    "i\n",
    "​\t w \n",
    "i\n",
    "​\t x \n",
    "i\n",
    "​\t )\n",
    "Update the weight step \\Delta w_i = \\Delta w_i + \\delta x_iΔw \n",
    "i\n",
    "​\t =Δw \n",
    "i\n",
    "​\t +δx \n",
    "i\n",
    "​\t \n",
    "Update the weights w_i = w_i + \\eta \\Delta w_i / mw \n",
    "i\n",
    "​\t =w \n",
    "i\n",
    "​\t +ηΔw \n",
    "i\n",
    "​\t /m where \\etaη is the learning rate and mm is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "Repeat for ee epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to do forward pass with hidden network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X,weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out,weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Forward and back propagation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target-output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error*output*(1-output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error=np.dot(output_error_term,weights_hidden_output)\n",
    "\n",
    "hidden_error_term=np.dot(output_error_term,weights_hidden_output)*hidden_layer_output*(1-hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o =learnrate* hidden_layer_output*output_error_term\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate *x[:,None] *hidden_error_term\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Here's the general algorithm for updating the weights with backpropagation:\n",
    "\n",
    "Set the weight steps for each layer to zero\n",
    "\n",
    "    The input to hidden weights \\Delta w_{ij} = 0Δw ij =0\n",
    "\n",
    "    The hidden to output weights \\Delta W_j = 0ΔW j =0\n",
    "\n",
    "\n",
    "For each record in the training data:\n",
    "\n",
    "    Make a forward pass through the network, calculating the output y^\n",
    "\t \n",
    "    Calculate the error gradient(error_term) in the output unit, d^o=(y− y^)f ′(z) where z = \\sum_j W_j a_jz=∑ j W j\t a j\t , the input to the output unit.\n",
    "    Propagate the errors to the hidden layer d^h_j = d^o W_j f'(h_j)\n",
    "    Update the weight steps:\n",
    "\n",
    "        delta_W_j = delta_W_j + d^o a_j\n",
    "        Delta_w_ij = delta_w_{ij} +d^h_j a_i \n",
    "        \n",
    "    Update the weights, where a is the learning rate and mm is the number of records:\n",
    "        W_j = W j\t +aΔW j\t /m\n",
    "        w_{ij} = w ij\t +aΔw ij\t /m\n",
    "Repeat for ee epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'data_prep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3a81b43407b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_prep\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'data_prep'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output,weights_hidden_output))\n",
    "        \n",
    "        \n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y-output\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error*output*(1-output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term,weights_hidden_output)\n",
    "        \n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error*hidden_output*(1-hidden_output)\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term*hidden_output\n",
    "        del_w_input_hidden += hidden_error_term*x[:,None]\n",
    "\n",
    "    # TODO: Update weights  (don't forget to division by n_records or number of samples)\n",
    "    weights_input_hidden += learnrate*del_w_input_hidden/n_records\n",
    "    weights_hidden_output +=learnrate*del_w_hidden_output/n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "dropout\n",
    "\n",
    "    -while training neural network. Some nodes are made unavaible during some epochs to give each node a chance to add to the algorithm\n",
    "    -we give each node a probability to turn off (dropout)\n",
    "\n",
    "alternate activation function\n",
    "\n",
    "    -tanh(x)=(exp(x)-exp(-x))/(exp(x)+exp(-x))\n",
    "    -relu(x)=if num positive then return 1 elif neg return 0\n",
    "\n",
    "stochaistic gradient descent\n",
    "\n",
    "    -subset data in multiple batches then train\n",
    "    \n",
    "learning rate-\n",
    "    if too high model my miss local minima\n",
    "    \n",
    "momentum - b,b^2,b^3,b^4\n",
    "    previous steps are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
