{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ensemble methods\n",
    "    bagging - take average of answers\n",
    "    boosting - give priority or weight to features that do well\n",
    "    weak learners put together to make a strong learner\n",
    "    ensemble to combing\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "variables\n",
    "    bias\n",
    "        high bias not good at bending to the data\n",
    "    variance\n",
    "        high variance allows for decision trees to make a seperate leaf for each point\n",
    "        \n",
    "combining algorithms is good to find a good center for bias and variance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "introduction of randomness\n",
    "    bootstrap - sample data with replacement\n",
    "    subset the features - subset a portion of the features for each split of a decision tree or algorithm\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "random forests\n",
    "    takes only a subset of the features to make a seperate prediction and then provides prediction based on most common prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adaboost\n",
    "    making weights higher of misclassified points after making split. Make weights higher to be evenly weighted split. Resplit\n",
    "    Keep doing splits. Then take all the splits into account for adaboost split of ensemble.\n",
    "    \n",
    "    each model will calculate the weight and then those weights will be combined when voting for classifications\n",
    "    weight=ln(accuracy/1-accuracy)\n",
    "    \n",
    "    hyperparameters\n",
    "        base_estimator - utilized for weak learners(ex. decision tree classifiers)\n",
    "        n_estimators: max number of weak learners used\n",
    "        ex:\n",
    "            from sklearn.tree import DecisionTreeClassifier\n",
    "            model=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_mod_acc=2/8\n",
    "(first_mod_acc/(1-first_mod_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0986122886681098"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_mod=log(first_mod_acc/(1-first_mod_acc))\n",
    "first_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "# ex.\n",
    "# Instantiate a BaggingClassifier with:\n",
    "# 200 weak learners (n_estimators) and everything else as default values\n",
    "clf1=BaggingClassifier(n_estimators=200)\n",
    "\n",
    "\n",
    "# Instantiate a RandomForestClassifier with:\n",
    "# 200 weak learners (n_estimators) and everything else as default values\n",
    "clf2=RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Instantiate an a AdaBoostClassifier with:\n",
    "# With 300 weak learners (n_estimators) and a learning_rate of 0.2\n",
    "clf3=AdaBoostClassifier(n_estimators=300)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "High Bias, Low Variance models tend to underfit data, as they are not flexible. Linear models fall into this category of models.\n",
    "\n",
    "High Variance, Low Bias models tend to overfit data, as they are too flexible. Decision trees fall into this category of models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
